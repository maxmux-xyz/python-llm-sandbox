{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/data-science-collective/the-complete-guide-to-building-your-first-ai-agent-its-easier-than-you-think-c87f376c84b2\n",
    "\n",
    "What are some good cheap models for classification?\n",
    "\n",
    "I want to classify text (youtube transcripts or articles) - in different categories (finance, crypto, education, news). \n",
    "- And subsequently I want to summarize the text. \n",
    "- And subsequently I want to extract all personas and their opinions / sentiment. \n",
    "- And subsequently I want to extract all entities (organisations / companies) mentionned in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "  model=\"deepseek/deepseek-r1\",\n",
    "  temperature=0,\n",
    "  openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Yes, I'm up and running. How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Hello! Are you working?\") \n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    text: str  # Stores the original input text\n",
    "    classification: str  # Represents the classification result (e.g., category label)\n",
    "    human_entities: List[str]  # Holds a list of extracted human entities (e.g., named entities)\n",
    "    company_entities: List[str]  # Holds a list of extracted company entities (e.g., named entities)\n",
    "    summary: str  # Stores a summarized version of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_node(state: State):\n",
    "   \"\"\"\n",
    "   Classify the text into one of predefined categories.\n",
    "   \n",
    "   Parameters:\n",
    "       state (State): The current state dictionary containing the text to classify\n",
    "       \n",
    "   Returns:\n",
    "       dict: A dictionary with the \"classification_type\" key containing the category result\n",
    "       \n",
    "   Categories:\n",
    "       - article: Factual reporting of current events\n",
    "       - youtube_transcript: Personal or informal web writing\n",
    "       - Other: Content that doesn't fit the above categories\n",
    "   \"\"\"\n",
    "\n",
    "   instructions = \"\"\"\n",
    "   You are a helpful assistant that can classify youtube transcripts into one of the following categories:\n",
    "   - News: Factual reporting of current events\n",
    "   - Podcast: Discussion between hosts, about one or more subjects\n",
    "   - Finance: Discussion about finance, markets, economics, etc. Could be one or multiple hosts.\n",
    "   - Education: Discuss about education, learning, teaching, etc. Could be one or multiple hosts.\n",
    "   - other: Content that doesn't fit the above categories\n",
    "\n",
    "   return 'news', 'podcast', 'finance', 'education', or 'other' - no other words or characters.\n",
    "   \"\"\"\n",
    "\n",
    "   # Define a prompt template that asks the model to classify the given text\n",
    "   prompt = PromptTemplate(\n",
    "       input_variables=[\"text\"],\n",
    "       template= instructions + \"\\n\\nText: {text}\\n\\nCategory:\"\n",
    "   )\n",
    "\n",
    "   # Format the prompt with the input text from the state\n",
    "   message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "\n",
    "   # Invoke the language model to classify the text based on the prompt\n",
    "   classification = llm.invoke([message]).content.strip()\n",
    "\n",
    "   # Return the classification result in a dictionary\n",
    "   return {\"classification\": classification}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_entity_extraction_node(state: State):\n",
    "\n",
    "  instructions = \"\"\"\n",
    "  Extract all the Human entities from the following transcript. \n",
    "  Provide the result as a comma-separated list.\n",
    "\n",
    "  The output should be a comma-separated list of human entities, no other text or characters.\n",
    "  Parsable by a python script.\n",
    "  \"\"\"\n",
    "  prompt = PromptTemplate(\n",
    "      input_variables=[\"text\"],\n",
    "      template= instructions + \"\\n\\nText: {text}\\n\\nHuman Entities:\"\n",
    "  )\n",
    "  \n",
    "  message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "  \n",
    "  human_entities = llm.invoke([message]).content.strip().split(\", \")\n",
    "  \n",
    "  # Return dictionary with entities list to be merged into agent state\n",
    "  return {\"human_entities\": human_entities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_entity_extraction_node(state: State):\n",
    "\n",
    "  instructions = \"\"\"\n",
    "  Extract all the Company entities from the following transcript. \n",
    "  Provide the result as a comma-separated list.\n",
    "\n",
    "  The output should be a comma-separated list of company entities, no other text or characters.\n",
    "  Parsable by a python script.\n",
    "  \"\"\"\n",
    "  prompt = PromptTemplate(\n",
    "      input_variables=[\"text\"],\n",
    "      template= instructions + \"\\n\\nText: {text}\"\n",
    "  )\n",
    "  \n",
    "  message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "  \n",
    "  company_entities = llm.invoke([message]).content.strip().split(\", \")\n",
    "  \n",
    "  # Return dictionary with entities list to be merged into agent state\n",
    "  return {\"company_entities\": company_entities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_node(state: State):\n",
    "    # Create a template for the summarization prompt\n",
    "    # This tells the model to summarize the input text in one sentence\n",
    "    summarization_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"You are a helpful assistant that summarizes transcripts of youtube videos. \n",
    "        Summarize the transcript in a way that is easy to understand and contains the most important information. \n",
    "        - First section should be a high level summary - aim for 3-5 concise bullet points.\n",
    "        - Second section should be to list the speakers and the main points they make.\n",
    "        - Third section should be a list of topics that are discussed.\n",
    "        - Fourth section should be to flag if any stock tickers or cryptocurrencies are mentioned. If so, what was the sentiment on them?\n",
    "        - Fifth section should be about markout outlook. Are speakers bullish or bearish - short or long term? Only if transcript is about markets or trading.\n",
    "        - Sixth section should be to flag any other interesting information that is not covered in the other sections, and / or perhpas one or a few meanigful quotes from any of the speakers.\n",
    "        Keep it short and concise, each section should be well apparent and contain up to 10 bullet points.\n",
    "        \n",
    "        Text: {text}\n",
    "        \n",
    "        Summary:\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create a chain by connecting the prompt template to the language model\n",
    "    # The \"|\" operator pipes the output of the prompt into the model\n",
    "    chain = summarization_prompt | llm\n",
    "    \n",
    "    # Execute the chain with the input text from the state dictionary\n",
    "    # This passes the text to be summarized to the model\n",
    "    response = chain.invoke({\"text\": state[\"text\"]})\n",
    "    \n",
    "    # Return a dictionary with the summary extracted from the model's response\n",
    "    # This will be merged into the agent's state\n",
    "    return {\"summary\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"classification_node\", classification_node)\n",
    "workflow.add_node(\"human_entity_extraction\", human_entity_extraction_node)\n",
    "workflow.add_node(\"company_entity_extraction\", company_entity_extraction_node)\n",
    "workflow.add_node(\"summarization\", summarize_node)\n",
    "\n",
    "# Add edges to the graph\n",
    "workflow.set_entry_point(\"classification_node\") # Set the entry point of the graph\n",
    "workflow.add_edge(\"classification_node\", \"human_entity_extraction\")\n",
    "workflow.add_edge(\"human_entity_extraction\", \"company_entity_extraction\")\n",
    "workflow.add_edge(\"company_entity_extraction\", \"summarization\")\n",
    "workflow.add_edge(\"summarization\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Load the sample transcript.json\n",
    "with open(\"./data/transcript.json\", \"r\") as f:\n",
    "    sample_text = json.load(f)\n",
    "\n",
    "# Convert the transcript to a string\n",
    "sample_text = \"\\n\".join([item[\"text\"] for item in sample_text])\n",
    "\n",
    "# Create the initial state with our sample text\n",
    "state_input = {\"text\": sample_text}\n",
    "\n",
    "# Run the agent's full workflow on our sample text\n",
    "result = app.invoke(state_input)\n",
    "\n",
    "print(\"DONE!\")\n",
    "\n",
    "# Print each component of the result:\n",
    "# - The classification category ('news', 'podcast', 'finance', 'education', or 'other')\n",
    "# print(\"Classification:\", result[\"classification\"])\n",
    "\n",
    "# - The extracted human entities:\n",
    "# print(\"\\nHuman Entities:\", result[\"human_entities\"])\n",
    "\n",
    "# - The extracted company entities:\n",
    "# print(\"\\nCompany Entities:\", result[\"company_entities\"])\n",
    "\n",
    "# - The generated summary of the text\n",
    "# print(\"\\nSummary:\", result[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result to file\n",
    "with open(\"./data/result.json\", \"w\") as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
